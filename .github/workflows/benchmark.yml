name: Benchmark

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]
  workflow_dispatch:  # Allow manual triggering

jobs:
  benchmark:
    name: Run benchmarks
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install torch --index-url https://download.pytorch.org/whl/cpu

    - name: Verify numba installation
      run: |
        python -c "import numba; print(f'numba version: {numba.__version__}')"
        python -c "import gsmod; print(f'gsmod version: {gsmod.__version__}')"

    - name: Create test data
      run: |
        python -c "
        import numpy as np
        from pathlib import Path
        import gsply

        # Create directory structure expected by benchmarks
        test_dir = Path('export_with_edits')
        test_dir.mkdir(exist_ok=True)

        # Generate synthetic Gaussian data
        num_gaussians = 100000
        np.random.seed(42)  # Reproducible data
        means = np.random.randn(num_gaussians, 3).astype(np.float32) * 10
        scales = np.abs(np.random.randn(num_gaussians, 3).astype(np.float32))
        quats = np.random.randn(num_gaussians, 4).astype(np.float32)
        quats = quats / np.linalg.norm(quats, axis=1, keepdims=True)
        opacities = np.random.randn(num_gaussians).astype(np.float32)
        sh0 = np.random.randn(num_gaussians, 3).astype(np.float32)
        shN = np.random.randn(num_gaussians, 15, 3).astype(np.float32)

        # Write test files
        test_file = test_dir / 'frame_00000.ply'
        gsply.plywrite(str(test_file), means, scales, quats, opacities, sh0, shN)
        print(f'Created test file: {test_file} ({test_file.stat().st_size / 1024 / 1024:.2f}MB)')

        # Also create a direct test file
        gsply.plywrite('test_data.ply', means, scales, quats, opacities, sh0, shN)
        "

    - name: Run color benchmark
      run: |
        cd benchmarks
        python benchmark_color_optimization.py 2>&1 | tee ../benchmark_color_results.txt
      continue-on-error: true

    - name: Run learnable benchmark
      run: |
        cd benchmarks
        python benchmark_learnable_cpu.py 2>&1 | tee ../benchmark_learnable_results.txt
      continue-on-error: true

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          benchmark_color_results.txt
          benchmark_learnable_results.txt

    - name: Comment benchmark results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v8
      with:
        script: |
          const fs = require('fs');
          let results = '';
          try {
            results += '### Color Benchmark\n\n```\n';
            results += fs.readFileSync('benchmark_color_results.txt', 'utf8');
            results += '\n```\n\n';
          } catch (e) {
            results += 'Color benchmark failed\n\n';
          }
          try {
            results += '### Learnable Benchmark\n\n```\n';
            results += fs.readFileSync('benchmark_learnable_results.txt', 'utf8');
            results += '\n```\n\n';
          } catch (e) {
            results += 'Learnable benchmark failed\n\n';
          }
          const body = `## Benchmark Results\n\n${results}`;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });
      continue-on-error: true
